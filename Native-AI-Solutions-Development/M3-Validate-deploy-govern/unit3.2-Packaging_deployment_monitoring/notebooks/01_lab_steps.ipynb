{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a208f0dd-4291-482c-babf-02f139da0068",
   "metadata": {},
   "source": [
    "# Unit 3.2 – Deployment and Monitoring with Lightweight CI/CD (MLOps-lite)\n",
    "## Jupyter Lab Notebook\n",
    "\n",
    "This notebook simulates a lightweight CI/CD workflow for Native AI systems without Docker or external CI platforms.\n",
    "We implement pipeline logic step-by-step in Python so that quality gates, artifact packaging, monitoring signals, and rollback rules remain transparent and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5010bccd-3a71-4b38-a2f2-2241d30bd46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: C:\\Users\\ikybe\\5g-digits\\unit32\n",
      "SRC_DIR: C:\\Users\\ikybe\\5g-digits\\unit32\\src\n",
      "REPORTS_DIR: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\reports\n",
      "RELEASES_DIR: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\releases\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Project root = parent of notebooks/\n",
    "ROOT = Path.cwd().parent\n",
    "\n",
    "SRC_DIR = ROOT / \"src\"\n",
    "ARTIFACTS_DIR = ROOT / \"artifacts\"\n",
    "REPORTS_DIR = ARTIFACTS_DIR / \"reports\"\n",
    "RELEASES_DIR = ARTIFACTS_DIR / \"releases\"\n",
    "\n",
    "for p in [SRC_DIR, REPORTS_DIR, RELEASES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"SRC_DIR:\", SRC_DIR)\n",
    "print(\"REPORTS_DIR:\", REPORTS_DIR)\n",
    "print(\"RELEASES_DIR:\", RELEASES_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c7b00-9ba8-46e3-b095-11b2d7178a63",
   "metadata": {},
   "source": [
    "## Lab Step 1: Implement Quality Gates from Benchmarks\n",
    "\n",
    "Goal: Convert benchmarking results into automated **pass/fail** decisions (quality gates).\n",
    "\n",
    "In a real CI pipeline, this step would run automatically on every update.\n",
    "Here, we simulate it locally and generate:\n",
    "- a KPI report (JSON)\n",
    "- a gate decision (PASS/FAIL)\n",
    "- a short human-readable summary\n",
    "\n",
    "We will use *illustrative KPIs* that align with edge AI constraints:\n",
    "- Accuracy (higher is better)\n",
    "- Latency (lower is better)\n",
    "- Model size (lower is better)\n",
    "- Validation failure rate (lower is better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ede888-8101-4104-84cd-c48500b55884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate decision: PASS ✅\n",
      "Report saved to: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\reports\\gate_report_20251215_194241.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, str(ROOT))  # allow \"from src...\" imports\n",
    "\n",
    "from src.gates import GateThresholds, evaluate_gates\n",
    "from src.utils_io import timestamp_id, write_json\n",
    "\n",
    "# Synthetic KPIs (we will later replace with actual measurements if desired)\n",
    "kpis = {\n",
    "    \"accuracy\": 0.88,\n",
    "    \"latency_ms\": 22.4,\n",
    "    \"model_size_mb\": 4.2,\n",
    "    \"validation_fail_rate\": 0.01\n",
    "}\n",
    "\n",
    "thresholds = GateThresholds(\n",
    "    min_accuracy=0.85,\n",
    "    max_latency_ms=25.0,\n",
    "    max_model_size_mb=5.0,\n",
    "    max_validation_fail_rate=0.02\n",
    ")\n",
    "\n",
    "overall_pass, gate_details = evaluate_gates(kpis, thresholds)\n",
    "\n",
    "run_id = timestamp_id()\n",
    "report = {\n",
    "    \"run_id\": run_id,\n",
    "    \"kpis\": kpis,\n",
    "    \"thresholds\": thresholds.__dict__,\n",
    "    \"gate_details\": gate_details\n",
    "}\n",
    "\n",
    "report_path = REPORTS_DIR / f\"gate_report_{run_id}.json\"\n",
    "write_json(report_path, report)\n",
    "\n",
    "print(\"Gate decision:\", \"PASS ✅\" if overall_pass else \"FAIL ❌\")\n",
    "print(\"Report saved to:\", report_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7bfe9-ecdb-4cc5-95b8-7326a2c2abac",
   "metadata": {},
   "source": [
    "Print a readable summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01b96e5-bc56-437d-9fb5-768e7c091b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quality Gate Summary\n",
      "------------------------------------------------------------\n",
      "accuracy               PASS | value=0.88 | threshold=0.85 | higher_is_better\n",
      "latency_ms             PASS | value=22.4 | threshold=25.0 | lower_is_better\n",
      "model_size_mb          PASS | value=4.2 | threshold=5.0 | lower_is_better\n",
      "validation_fail_rate   PASS | value=0.01 | threshold=0.02 | lower_is_better\n",
      "------------------------------------------------------------\n",
      "Overall: PASS ✅\n"
     ]
    }
   ],
   "source": [
    "checks = gate_details[\"checks\"]\n",
    "\n",
    "print(\"\\nQuality Gate Summary\")\n",
    "print(\"-\" * 60)\n",
    "for name, info in checks.items():\n",
    "    status = \"PASS\" if info[\"pass\"] else \"FAIL\"\n",
    "    print(f\"{name:22s} {status:4s} | value={info['value']} | threshold={info['threshold']} | {info['direction']}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Overall:\", \"PASS ✅\" if gate_details[\"overall_pass\"] else \"FAIL ❌\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e9e7c-2cb5-4357-b1f9-0ac0f9b942c5",
   "metadata": {},
   "source": [
    "## Lab Step 2: Package a Release Artifact (MLOps-lite)\n",
    "\n",
    "Goal: create a simple, reproducible “release bundle” that could be deployed on an edge device.\n",
    "\n",
    "In real CI/CD, a successful gate decision would automatically trigger packaging.  \n",
    "Here, we simulate this by building a release folder containing:\n",
    "\n",
    "- `manifest.json` (metadata: run_id, KPIs, gate result, versions)\n",
    "- `gate_report_*.json` (the full gate report)\n",
    "- `README.txt` (human-friendly deployment notes)\n",
    "\n",
    "This step does **not** require Docker or a real deployment platform; it focuses on the core idea:\n",
    "**only gated builds become deployable artifacts.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccf8323-fbed-4964-947d-49350e4a1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest report: gate_report_20251215_194241.json\n",
      "run_id: 20251215_194241\n",
      "Gate decision: PASS ✅\n",
      "write_text available: True\n"
     ]
    }
   ],
   "source": [
    "# 2.1 - Locate latest gate report and load it (with safe reload of src.utils_io)\n",
    "\n",
    "import json\n",
    "import importlib\n",
    "import src.utils_io as utils_io\n",
    "\n",
    "# Reload to ensure Jupyter picks up your latest edits\n",
    "importlib.reload(utils_io)\n",
    "\n",
    "write_text = utils_io.write_text  # now guaranteed to exist if it's in the file\n",
    "\n",
    "# Find the most recent gate report in artifacts/reports\n",
    "reports = sorted(REPORTS_DIR.glob(\"gate_report_*.json\"), reverse=True)\n",
    "assert len(reports) > 0, f\"No gate reports found in {REPORTS_DIR}\"\n",
    "\n",
    "latest_report_path = reports[0]\n",
    "with open(latest_report_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    latest_report = json.load(f)\n",
    "\n",
    "run_id = latest_report[\"run_id\"]\n",
    "overall_pass = latest_report[\"gate_details\"][\"overall_pass\"]\n",
    "\n",
    "print(\"Latest report:\", latest_report_path.name)\n",
    "print(\"run_id:\", run_id)\n",
    "print(\"Gate decision:\", \"PASS ✅\" if overall_pass else \"FAIL ❌\")\n",
    "\n",
    "# 2.2. Quick sanity check (should be 'True')\n",
    "print(\"write_text available:\", hasattr(utils_io, \"write_text\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53418eb-265a-4e38-b858-5e0380e6b5c1",
   "metadata": {},
   "source": [
    "### Packaging rule\n",
    "\n",
    "We only create a release artifact when the gate decision is **PASS**.\n",
    "If the decision is **FAIL**, the pipeline stops here (no deployable bundle is produced).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485ac985-1909-4659-aba7-97379bc1a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release bundle created ✅\n",
      "Release directory: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\releases\\release_20251215_194241\n",
      "Files: ['gate_report_20251215_194241.json', 'manifest.json', 'README.txt']\n"
     ]
    }
   ],
   "source": [
    "# 2.3 -  Release folder (one per run_id)\n",
    "release_dir = RELEASES_DIR / f\"release_{run_id}\"\n",
    "\n",
    "if not overall_pass:\n",
    "    print(\"Gate decision is FAIL ❌ — skipping packaging.\")\n",
    "else:\n",
    "    release_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Copy the gate report into the release bundle\n",
    "    gate_copy_path = release_dir / latest_report_path.name\n",
    "    gate_copy_path.write_text(json.dumps(latest_report, indent=2, sort_keys=True), encoding=\"utf-8\")\n",
    "\n",
    "    # 2) Create a short manifest.json (minimal deployment metadata)\n",
    "    manifest = {\n",
    "        \"run_id\": run_id,\n",
    "        \"created_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"gate_decision\": \"PASS\",\n",
    "        \"kpis\": latest_report[\"kpis\"],\n",
    "        \"thresholds\": latest_report[\"thresholds\"],\n",
    "        \"notes\": \"Release created by Unit 3.2 notebook pipeline (MLOps-lite).\"\n",
    "    }\n",
    "    manifest_path = release_dir / \"manifest.json\"\n",
    "    manifest_path.write_text(json.dumps(manifest, indent=2, sort_keys=True), encoding=\"utf-8\")\n",
    "\n",
    "    # 3) Create a human-readable README\n",
    "    readme = f\"\"\"Unit 3.2 – Release Artifact\n",
    "\n",
    "run_id: {run_id}\n",
    "gate_decision: PASS\n",
    "\n",
    "Included files:\n",
    "- {latest_report_path.name} : full quality gate report\n",
    "- manifest.json             : compact metadata for deployment tooling\n",
    "- README.txt                : this file\n",
    "\n",
    "Deployment note:\n",
    "This bundle is a simulation of a CI/CD release artifact. In a real system, it would also include\n",
    "model binaries, configuration, and versioned dependencies.\n",
    "\"\"\"\n",
    "    readme_path = release_dir / \"README.txt\"\n",
    "    write_text(readme_path, readme)\n",
    "\n",
    "    print(\"Release bundle created ✅\")\n",
    "    print(\"Release directory:\", release_dir)\n",
    "    print(\"Files:\", [p.name for p in sorted(release_dir.iterdir())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8505089-1ac2-4731-9fb8-819bfb7c2c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manifest check\n",
      "------------------------------------------------------------\n",
      "run_id: 20251215_194241\n",
      "gate_decision: PASS\n",
      "kpis: {'accuracy': 0.88, 'latency_ms': 22.4, 'model_size_mb': 4.2, 'validation_fail_rate': 0.01}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2.4 - Sanity check: read manifest back\n",
    "if overall_pass:\n",
    "    with open(release_dir / \"manifest.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        m = json.load(f)\n",
    "\n",
    "    print(\"\\nManifest check\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"run_id:\", m[\"run_id\"])\n",
    "    print(\"gate_decision:\", m[\"gate_decision\"])\n",
    "    print(\"kpis:\", m[\"kpis\"])\n",
    "    print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"No manifest created because packaging was skipped (FAIL gate).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4bcfe-7247-4c93-9135-457fc2a9403d",
   "metadata": {},
   "source": [
    "### Optional: Human-Readable Release Summary \n",
    "\n",
    "Some release manifests may not explicitly include a `gate_report` field.\n",
    "To keep this step robust, we:\n",
    "\n",
    "1. Read `manifest.json`\n",
    "2. Try to load the gate report referenced by the manifest\n",
    "3. If missing, auto-detect `gate_report_*.json` inside the release directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b46b643-82d4-4a26-8fee-cacd290c1408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Release Summary\n",
      "------------------------------------------------------------\n",
      "Release dir: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\releases\\release_20251215_194241\n",
      "run_id: 20251215_194241\n",
      "gate_decision: PASS\n",
      "kpis: {'accuracy': 0.88, 'latency_ms': 22.4, 'model_size_mb': 4.2, 'validation_fail_rate': 0.01}\n",
      "\n",
      "Gate report: gate_report_20251215_194241.json\n",
      "\n",
      "Checks:\n",
      "  - accuracy: PASS | value=0.88 | threshold=0.85 | higher_is_better\n",
      "  - latency_ms: PASS | value=22.4 | threshold=25.0 | lower_is_better\n",
      "  - model_size_mb: PASS | value=4.2 | threshold=5.0 | lower_is_better\n",
      "  - validation_fail_rate: PASS | value=0.01 | threshold=0.02 | lower_is_better\n",
      "\n",
      "Overall: PASS ✅\n"
     ]
    }
   ],
   "source": [
    "# 2.5 human-redable summary\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nRelease Summary\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Release dir:\", release_dir)\n",
    "\n",
    "manifest_path = release_dir / \"manifest.json\"\n",
    "assert manifest_path.exists(), f\"manifest.json not found in {release_dir}\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "print(\"run_id:\", manifest.get(\"run_id\"))\n",
    "print(\"gate_decision:\", manifest.get(\"gate_decision\"))\n",
    "print(\"kpis:\", manifest.get(\"kpis\", {}))\n",
    "\n",
    "# --- Resolve gate report path (preferred: manifest pointer; fallback: autodetect) ---\n",
    "gate_report_path = None\n",
    "\n",
    "gate_report_name = manifest.get(\"gate_report\")\n",
    "if isinstance(gate_report_name, str) and gate_report_name.strip():\n",
    "    candidate = release_dir / gate_report_name\n",
    "    if candidate.exists():\n",
    "        gate_report_path = candidate\n",
    "\n",
    "if gate_report_path is None:\n",
    "    # Fallback: locate gate_report_*.json in the release folder\n",
    "    candidates = sorted(release_dir.glob(\"gate_report_*.json\"), reverse=True)\n",
    "    if len(candidates) > 0:\n",
    "        gate_report_path = candidates[0]\n",
    "\n",
    "if gate_report_path is None:\n",
    "    print(\"\\nNo gate report found in release folder.\")\n",
    "else:\n",
    "    print(\"\\nGate report:\", gate_report_path.name)\n",
    "\n",
    "    with open(gate_report_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        gate_report = json.load(f)\n",
    "\n",
    "    gate_details = gate_report.get(\"gate_details\", {})\n",
    "    checks = gate_details.get(\"checks\", {})\n",
    "\n",
    "    print(\"\\nChecks:\")\n",
    "    for name, info in checks.items():\n",
    "        status = \"PASS\" if info.get(\"pass\") else \"FAIL\"\n",
    "        value = info.get(\"value\")\n",
    "        threshold = info.get(\"threshold\")\n",
    "        direction = info.get(\"direction\")\n",
    "        print(f\"  - {name}: {status} | value={value} | threshold={threshold} | {direction}\")\n",
    "\n",
    "    print(\"\\nOverall:\", \"PASS ✅\" if gate_details.get(\"overall_pass\") else \"FAIL ❌\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969be76b-bf01-4b72-b202-e4521e1dba29",
   "metadata": {},
   "source": [
    "## LAB STEP 3: Deployment Dry-Run (No Docker)\n",
    "\n",
    "In the real pipeline, a passing **quality gate** would trigger an automated deployment step.\n",
    "\n",
    "In this lab, we **simulate** that deployment without Docker or real infrastructure by:\n",
    "1. Creating a `deployment_plan.json` from the release manifest\n",
    "2. Running a “preflight” validation (files exist, metadata present)\n",
    "3. Producing a lightweight `deployment_receipt.json` that represents an executed deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f2ee64-2e02-4bad-b260-8e4ef510c5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment plan created ✅\n",
      "Plan saved to: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\deployments\\deploy_20251215_194241\\deployment_plan.json\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Create a deployment plan\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "DEPLOYMENTS_DIR = ARTIFACTS_DIR / \"deployments\"\n",
    "DEPLOYMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Inputs from previous steps:\n",
    "# - release_dir (Path)\n",
    "# - manifest (dict) OR manifest_path\n",
    "manifest_path = release_dir / \"manifest.json\"\n",
    "assert manifest_path.exists(), f\"manifest.json not found: {manifest_path}\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "run_id = manifest.get(\"run_id\") or release_dir.name.replace(\"release_\", \"\")\n",
    "deployment_id = f\"deploy_{run_id}\"\n",
    "deployment_dir = DEPLOYMENTS_DIR / deployment_id\n",
    "deployment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plan = {\n",
    "    \"deployment_id\": deployment_id,\n",
    "    \"created_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"source_release_dir\": str(release_dir),\n",
    "    \"run_id\": run_id,\n",
    "    \"gate_decision\": manifest.get(\"gate_decision\"),\n",
    "    \"kpis\": manifest.get(\"kpis\", {}),\n",
    "    \"artifacts\": {\n",
    "        \"manifest\": \"manifest.json\",\n",
    "        # optional field; may not exist in your manifest, but we keep it for completeness\n",
    "        \"gate_report\": manifest.get(\"gate_report\", None),\n",
    "    },\n",
    "    \"target\": {\n",
    "        \"environment\": \"staging\",\n",
    "        \"channel\": \"simulated\",\n",
    "        \"rollout\": \"100%\",\n",
    "    },\n",
    "}\n",
    "\n",
    "plan_path = deployment_dir / \"deployment_plan.json\"\n",
    "with open(plan_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(plan, f, indent=2, sort_keys=True)\n",
    "\n",
    "print(\"Deployment plan created ✅\")\n",
    "print(\"Plan saved to:\", plan_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dddaefb-7093-4031-90fd-5263a742a4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preflight check\n",
      "------------------------------------------------------------\n",
      "manifest.json           : True\n",
      "README.txt              : True\n",
      "gate_report_autodetect  : True\n",
      "------------------------------------------------------------\n",
      "Preflight: PASS ✅\n",
      "Gate report autodetect: gate_report_20251215_194241.json\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Preflight checks\n",
    "import os\n",
    "\n",
    "def preflight_check(release_dir: Path) -> dict:\n",
    "    required = [\"manifest.json\", \"README.txt\"]\n",
    "    present = {name: (release_dir / name).exists() for name in required}\n",
    "\n",
    "    # Gate report is optional (can be discovered later), so we don't fail if missing\n",
    "    gate_reports = sorted(release_dir.glob(\"gate_report_*.json\"), reverse=True)\n",
    "    present[\"gate_report_autodetect\"] = len(gate_reports) > 0\n",
    "\n",
    "    ok = all(present[name] for name in required)\n",
    "    return {\n",
    "        \"ok\": ok,\n",
    "        \"present\": present,\n",
    "        \"autodetected_gate_report\": gate_reports[0].name if gate_reports else None\n",
    "    }\n",
    "\n",
    "preflight = preflight_check(release_dir)\n",
    "\n",
    "print(\"\\nPreflight check\")\n",
    "print(\"-\" * 60)\n",
    "for k, v in preflight[\"present\"].items():\n",
    "    print(f\"{k:24s}: {v}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Preflight:\", \"PASS ✅\" if preflight[\"ok\"] else \"FAIL ❌\")\n",
    "print(\"Gate report autodetect:\", preflight[\"autodetected_gate_report\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b7ee84-adfd-458b-ad1d-eff61e970a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment result: DEPLOYED ✅\n",
      "Receipt saved to: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\deployments\\deploy_20251215_194241\\deployment_receipt.json\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Simulated deployment\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load plan back\n",
    "with open(plan_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    plan_loaded = json.load(f)\n",
    "\n",
    "# Only deploy if gate passed AND preflight passed\n",
    "gate_ok = (str(plan_loaded.get(\"gate_decision\")).upper() == \"PASS\")\n",
    "preflight_ok = bool(preflight.get(\"ok\"))\n",
    "\n",
    "deployed = gate_ok and preflight_ok\n",
    "\n",
    "receipt = {\n",
    "    \"deployment_id\": plan_loaded[\"deployment_id\"],\n",
    "    \"executed_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"status\": \"DEPLOYED\" if deployed else \"SKIPPED\",\n",
    "    \"reason\": None if deployed else {\n",
    "        \"gate_ok\": gate_ok,\n",
    "        \"preflight_ok\": preflight_ok\n",
    "    },\n",
    "    \"source_release_dir\": plan_loaded[\"source_release_dir\"],\n",
    "    \"target\": plan_loaded[\"target\"],\n",
    "}\n",
    "\n",
    "receipt_path = deployment_dir / \"deployment_receipt.json\"\n",
    "with open(receipt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(receipt, f, indent=2, sort_keys=True)\n",
    "\n",
    "print(\"Deployment result:\", receipt[\"status\"], (\"✅\" if deployed else \"⚠️\"))\n",
    "print(\"Receipt saved to:\", receipt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5c790-4b38-4ce2-bd26-93d56c92bff2",
   "metadata": {},
   "source": [
    "### 3.4 Post-deploy smoke check (simulated)\n",
    "\n",
    "After a deployment, teams usually run a small “smoke test” to confirm:\n",
    "- the deployment receipt exists and indicates `DEPLOYED`\n",
    "- the referenced release directory still exists\n",
    "- the manifest can be loaded and still reports a PASS gate decision\n",
    "\n",
    "This is not a functional test of the full system — it is a lightweight integrity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "833368ab-4fe0-4e97-bd8e-333bcae5e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke check\n",
      "------------------------------------------------------------\n",
      "deployment_id: deploy_20251215_194241\n",
      "status: DEPLOYED\n",
      "gate_decision (manifest): PASS\n",
      "------------------------------------------------------------\n",
      "Smoke check: PASS ✅\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Post-deploy smoke check\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# We assume these variables exist from prior cells:\n",
    "# - deployment_dir (Path)\n",
    "# - release_dir (Path)\n",
    "\n",
    "receipt_path = deployment_dir / \"deployment_receipt.json\"\n",
    "assert receipt_path.exists(), f\"Missing receipt: {receipt_path}\"\n",
    "\n",
    "with open(receipt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    receipt = json.load(f)\n",
    "\n",
    "print(\"Smoke check\")\n",
    "print(\"-\" * 60)\n",
    "print(\"deployment_id:\", receipt.get(\"deployment_id\"))\n",
    "print(\"status:\", receipt.get(\"status\"))\n",
    "\n",
    "assert receipt.get(\"status\") == \"DEPLOYED\", \"Smoke check failed: deployment was not DEPLOYED\"\n",
    "assert Path(receipt.get(\"source_release_dir\")).exists(), \"Smoke check failed: source release dir missing\"\n",
    "\n",
    "manifest_path = release_dir / \"manifest.json\"\n",
    "assert manifest_path.exists(), f\"Missing manifest: {manifest_path}\"\n",
    "\n",
    "with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "print(\"gate_decision (manifest):\", manifest.get(\"gate_decision\"))\n",
    "assert str(manifest.get(\"gate_decision\")).upper() == \"PASS\", \"Smoke check failed: gate_decision is not PASS\"\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Smoke check: PASS ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b58fe-46d3-4deb-be55-79371cc3b311",
   "metadata": {},
   "source": [
    "### 3.5 Optional rollback simulation (simulated)\n",
    "\n",
    "If a smoke test fails in production, the common response is to:\n",
    "- mark the deployment as rolled back\n",
    "- preserve the original receipt for auditability\n",
    "\n",
    "Here we simulate a rollback by writing a `rollback_receipt.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03e289da-b144-4a28-9320-6c06c9734a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollback receipt written ⚠️\n",
      "Path: C:\\Users\\ikybe\\5g-digits\\unit32\\artifacts\\deployments\\deploy_20251215_194241\\rollback_receipt.json\n"
     ]
    }
   ],
   "source": [
    "# 3.5 Optional rollback simulation\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "rollback = {\n",
    "    \"deployment_id\": receipt.get(\"deployment_id\"),\n",
    "    \"rolled_back_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"status\": \"ROLLED_BACK\",\n",
    "    \"reason\": \"simulated_rollback_for_training\",\n",
    "    \"previous_status\": receipt.get(\"status\"),\n",
    "}\n",
    "\n",
    "rollback_path = deployment_dir / \"rollback_receipt.json\"\n",
    "with open(rollback_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rollback, f, indent=2, sort_keys=True)\n",
    "\n",
    "print(\"Rollback receipt written ⚠️\")\n",
    "print(\"Path:\", rollback_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47dbb83-d8e3-4fe1-83ce-6994f1ae145b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

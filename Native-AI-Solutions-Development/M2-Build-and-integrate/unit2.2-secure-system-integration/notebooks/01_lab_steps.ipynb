{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aea572a",
   "metadata": {},
   "source": [
    "# Unit 2.2 - Secure System Integration\n",
    "\n",
    "## Lab Step 1: Load the optimized model securely \n",
    "\n",
    "In this first lab step, you will:\n",
    "\n",
    "1. Load the optimized HAR model (pruned + quantized) using ONNX Runtime.  \n",
    "2. Inspect the model’s input and output tensors.  \n",
    "3. Run a minimal “smoke test” inference to confirm that the model is functional.\n",
    "\n",
    "This step establishes trust in the model artifact before we build the secure inference pipeline around it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644826b-141b-4638-93b1-9953e678d3c2",
   "metadata": {},
   "source": [
    "### 1.1 - Task 1: Import dependencies and load the model\n",
    "\n",
    "In this task you will:\n",
    "\n",
    "- Configure the Python path so the notebook can import modules from the `src` folder.\n",
    "- Use the `load_model` helper function to load the optimized ONNX model.\n",
    "- Confirm that the ONNX Runtime session is created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28cee87d-44bc-4cf7-92fb-98a4fb0c15b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\ikybe\\5g-digits\\unit22\n",
      "Source dir  : C:\\Users\\ikybe\\5g-digits\\unit22\\src\n",
      "Models dir  : C:\\Users\\ikybe\\5g-digits\\unit22\\models\n",
      "Model path  : C:\\Users\\ikybe\\5g-digits\\unit22\\models\\har_pruned_quantized.onnx\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure that the `src` folder is on the Python path\n",
    "# (this assumes the notebook is inside the `notebooks` folder)\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\"..\"))\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, \"src\")\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.append(SRC_DIR)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Source dir  :\", SRC_DIR)\n",
    "print(\"Models dir  :\", MODELS_DIR)\n",
    "\n",
    "# Import the helper for loading ONNX models\n",
    "from load_model import load_model\n",
    "\n",
    "# Path to the optimized model (produced in Unit 2.1)\n",
    "MODEL_PATH = os.path.join(MODELS_DIR, \"har_pruned_quantized.onnx\")\n",
    "print(\"Model path  :\", MODEL_PATH)\n",
    "\n",
    "# Load the model using ONNX Runtime\n",
    "session, input_tensor, output_tensor = load_model(MODEPATH := MODEL_PATH)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15bd43-41d2-4491-bde7-499332eeae1c",
   "metadata": {},
   "source": [
    "### 1.2 - Task 2: Inspect input and output tensors\n",
    "\n",
    "Now that the model is loaded, we need to understand **how** it expects data:\n",
    "\n",
    "- What is the **input tensor name**?\n",
    "- What is the **input tensor shape** (batch size, window length, number of channels)?\n",
    "- What is the **data type** of the inputs and outputs?\n",
    "\n",
    "This information will be used later to implement the input validation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf47746-9b3c-449b-93d0-6438531809c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Input tensor ===\n",
      "Name : input\n",
      "Shape: ['unk__121', 100, 7]\n",
      "Type : tensor(float)\n",
      "\n",
      "=== Output tensor ===\n",
      "Name : dense_1\n",
      "Shape: ['unk__122', 8]\n",
      "Type : tensor(float)\n"
     ]
    }
   ],
   "source": [
    "# Inspect input tensor\n",
    "input_name = input_tensor.name\n",
    "input_shape = input_tensor.shape\n",
    "input_type = input_tensor.type\n",
    "\n",
    "# Inspect output tensor\n",
    "output_name = output_tensor.name\n",
    "output_shape = output_tensor.shape\n",
    "output_type = output_tensor.type\n",
    "\n",
    "print(\"=== Input tensor ===\")\n",
    "print(\"Name :\", input_name)\n",
    "print(\"Shape:\", input_shape)\n",
    "print(\"Type :\", input_type)\n",
    "print()\n",
    "print(\"=== Output tensor ===\")\n",
    "print(\"Name :\", output_name)\n",
    "print(\"Shape:\", output_shape)\n",
    "print(\"Type :\", output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ca0aa-c763-43c3-9a29-05ed6bb3622e",
   "metadata": {},
   "source": [
    "### 1.3 - Task 3: Run a minimal smoke test inference\n",
    "\n",
    "Before integrating the model into a secure pipeline, we perform a **smoke test**:\n",
    "\n",
    "- Create a dummy input window with the correct shape and data type.\n",
    "- Run a single inference through the model.\n",
    "- Check that the model returns a valid output tensor (no errors or exceptions).\n",
    "\n",
    "This confirms that the model file is not corrupted and that ONNX Runtime can execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e4b75d-d13a-4952-a9c3-8bef2f792b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoke test completed successfully.\n",
      "Output shape: (1, 8)\n",
      "Sample output (first row): [3.5683908e-02 9.3733311e-02 4.6272118e-02 5.2003423e-04 6.4002134e-02\n",
      " 5.4000902e-01 2.1881647e-01 9.6306583e-04]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Create a dummy input window based on the input shape\n",
    "# input_shape typically looks like: [None, window_length, num_channels]\n",
    "# We create a single batch (1, W, C) with reasonable IMU value ranges.\n",
    "\n",
    "batch_size = 1\n",
    "window_length = input_shape[1]\n",
    "num_channels = input_shape[2]\n",
    "\n",
    "dummy_window = np.zeros((batch_size, window_length, num_channels), dtype=np.float32)\n",
    "\n",
    "# Fill with small random values in a plausible IMU range, e.g. [-1.0, 1.0]\n",
    "dummy_window = np.random.uniform(-1.0, 1.0, size=dummy_window.shape).astype(np.float32)\n",
    "\n",
    "# Run a smoke test inference\n",
    "outputs = session.run([output_name], {input_name: dummy_window})\n",
    "predictions = outputs[0]\n",
    "\n",
    "print(\"Smoke test completed successfully.\")\n",
    "print(\"Output shape:\", predictions.shape)\n",
    "print(\"Sample output (first row):\", predictions[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c62789-ac63-49db-ba13-098c6e820f25",
   "metadata": {},
   "source": [
    "#### 1.3.1. Reflection\n",
    "\n",
    "If this smoke test fails (for example, due to a shape mismatch or runtime error),  \n",
    "what would be the most appropriate next step?\n",
    "\n",
    "- Inspect the model’s input shape and ensure the dummy window matches it.  \n",
    "- Check that the model file path is correct.  \n",
    "- Verify that ONNX Runtime is installed and working.\n",
    "\n",
    "These checks must be completed before proceeding to input validation and secure integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1d8dc-019b-4d88-b19c-ece126a1a1b6",
   "metadata": {},
   "source": [
    "## Lab Step 2 – Build the Secure Input Pipeline\n",
    "\n",
    "In this step, you will implement the fundamental validation layers required for secure integration of a model on an edge device. The goal is to ensure that only valid, meaningful, and physically plausible data reaches the inference engine.\n",
    "\n",
    "This step is divided into five parts:\n",
    "\n",
    "2.1 Shape Validation  \n",
    "2.2 Type Validation  \n",
    "2.3 Range Validation  \n",
    "2.4 Combined Validator  \n",
    "2.5 Testing Invalid Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9bc7cc-4f61-4797-89b5-351996b51b54",
   "metadata": {},
   "source": [
    "### 2.1 - Shape Validation\n",
    "\n",
    "The model expects input windows with a specific shape:\n",
    "(batch_size = 1, window_length, num_channels)\n",
    "\n",
    "This shape must match exactly what the model’s input tensor specifies.  \n",
    "If the shape is incorrect, ONNX Runtime will fail immediately.\n",
    "\n",
    "In this section, you will implement a function that verifies the shape of an input window before it reaches the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc8ef44-b0cb-45a1-8016-51e680a65d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected shape: ('unk__121', 100, 7)\n"
     ]
    }
   ],
   "source": [
    "from validation import validate_shape\n",
    "\n",
    "expected_shape = tuple(input_shape)  # From Lab Step 1\n",
    "\n",
    "def test_shape(window):\n",
    "    return validate_shape(window, expected_shape)\n",
    "\n",
    "print(\"Expected shape:\", expected_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39937ffc-ded1-4e4c-8e79-e3519589380b",
   "metadata": {},
   "source": [
    "### 2.2 – Type Validation\n",
    "\n",
    "Even if the shape is correct, the input window must also have the correct data type.  \n",
    "This model expects float32 (`np.float32`) inputs.\n",
    "\n",
    "Here you will implement type validation to ensure dtype consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fc783d-527b-49ff-93d2-ee2cac1c8001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected dtype: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "from validation import validate_dtype\n",
    "import numpy as np\n",
    "\n",
    "expected_dtype = np.float32\n",
    "\n",
    "def test_dtype(window):\n",
    "    return validate_dtype(window, expected_dtype)\n",
    "\n",
    "print(\"Expected dtype:\", expected_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc0375-1df8-42a1-8cf3-169af88165c8",
   "metadata": {},
   "source": [
    "### 2.3 - Range Validation\n",
    "\n",
    "Physical sensors such as IMUs operate within well-defined numerical limits.  \n",
    "Values outside the allowed range indicate noise, corruption, or sensor malfunction.\n",
    "\n",
    "For this model, we assume accelerometer and gyroscope values must remain within:\n",
    "[-4.0, +4.0]\n",
    "\n",
    "Here you will implement range validation to detect out-of-bound values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e1cdd7a-658b-482c-a8fc-9d9b8407d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid range: -4.0 to 4.0\n"
     ]
    }
   ],
   "source": [
    "from validation import validate_range\n",
    "\n",
    "min_val, max_val = -4.0, 4.0\n",
    "\n",
    "def test_range(window):\n",
    "    return validate_range(window, min_value=min_val, max_value=max_val)\n",
    "\n",
    "print(f\"Valid range: {min_val} to {max_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764b24d-8b69-4549-a976-22616a40020f",
   "metadata": {},
   "source": [
    "## 2.4 - Combined Validator (Master Function)\n",
    "\n",
    "Now that we have implemented shape, type, and range checks individually,  \n",
    "we will combine them into a single master validator.\n",
    "\n",
    "If any component fails, the whole validation fails.\n",
    "\n",
    "This validator will be used later in the secure inference wrapper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8bc411-3794-47c7-9e2b-358822e05149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master validator ready.\n"
     ]
    }
   ],
   "source": [
    "from validation import validate_input\n",
    "\n",
    "def master_validate(window):\n",
    "    return validate_input(\n",
    "        window,\n",
    "        expected_shape=expected_shape,\n",
    "        expected_dtype=expected_dtype\n",
    "    )\n",
    "\n",
    "print(\"Master validator ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8b8e7-9820-4997-bfca-7fa6094d4d5c",
   "metadata": {},
   "source": [
    "### 2.5 - Testing Invalid Cases\n",
    "\n",
    "A validator is only reliable if it rejects invalid inputs *consistently*.  \n",
    "In this section, you will test the validator with deliberately malformed inputs:\n",
    "\n",
    "- Wrong shapes  \n",
    "- Wrong dtype  \n",
    "- Out-of-range values  \n",
    "- NaN or infinite values  \n",
    "- Corrupted windows (missing elements)\n",
    "\n",
    "Note:\n",
    "ONNX models sometimes contain symbolic (non-numeric) dimension names such as \"unk__120\" instead of actual integers.\n",
    "This is normal: symbolic dimensions indicate that the model accepts variable-length input sizes.\n",
    "However, NumPy cannot create arrays using symbolic dimensions.\n",
    "Therefore, when generating synthetic test inputs, replace any symbolic dimension with a numeric placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6036e56-a6ca-4ab0-84cb-8a01d1493a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric shape used for validation tests: (1, 100, 7)\n",
      "Wrong shape: False\n",
      "Wrong dtype: False\n",
      "Out of range: False\n",
      "Contains NaN: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert expected_shape to usable numeric shape\n",
    "numeric_shape = tuple(\n",
    "    dim if isinstance(dim, int) else 1   # replace strings like \"unk__120\" by 1\n",
    "    for dim in expected_shape\n",
    ")\n",
    "\n",
    "print(\"Numeric shape used for validation tests:\", numeric_shape)\n",
    "\n",
    "invalid_shape = np.zeros((1, 50, 3), dtype=np.float32)  # wrong window length\n",
    "invalid_dtype = np.zeros(numeric_shape, dtype=np.float64)  # wrong dtype\n",
    "invalid_range = np.random.uniform(-10, 10, size=numeric_shape).astype(np.float32)\n",
    "invalid_nan = np.zeros(numeric_shape, dtype=np.float32); invalid_nan[0, 0, 0] = np.nan\n",
    "\n",
    "tests = {\n",
    "    \"Wrong shape\": invalid_shape,\n",
    "    \"Wrong dtype\": invalid_dtype,\n",
    "    \"Out of range\": invalid_range,\n",
    "    \"Contains NaN\": invalid_nan,\n",
    "}\n",
    "\n",
    "for name, w in tests.items():\n",
    "    print(f\"{name}: {master_validate(w)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df19f76-a274-47e6-ab6c-87d7ee10b0e4",
   "metadata": {},
   "source": [
    "## Lab Step 3:  Safe Inference Wrapper\n",
    "\n",
    "### 3.1 - What Is a Safe Inference Wrapper?\n",
    "\n",
    "In real embedded deployments, raw data cannot be trusted.  \n",
    "Sensors may fail, values may drift, and communication channels may introduce corruption.\n",
    "\n",
    "A **Safe Inference Wrapper** is a protective function that:\n",
    "\n",
    "- Validates the input tensor\n",
    "- Rejects unsafe inputs early\n",
    "- Logs structured errors\n",
    "- Only runs inference when validation succeeds\n",
    "\n",
    "This wrapper will become the main interface for secure model execution inside an embedded AI system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fc630-a783-4563-a5b6-72aece5d5f9b",
   "metadata": {},
   "source": [
    "### 3.2 – Implementing the Safe Inference Wrapper\n",
    "\n",
    "The safe inference wrapper will:\n",
    "\n",
    "1. Validate the input window using the combined validator.\n",
    "2. Only run inference if validation passes.\n",
    "3. Return a **structured dictionary** with:\n",
    "   - `ok` (True/False)\n",
    "   - `error` (string or None)\n",
    "   - `prediction` (class index or None)\n",
    "\n",
    "This function will be the main secure entry point for running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d0a491-9b47-4fd5-95a0-e914473a2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def secure_predict(session, input_name, output_name, window, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Safe inference wrapper for the optimized HAR model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    session : onnxruntime.InferenceSession\n",
    "        ONNX Runtime session for the model.\n",
    "    input_name : str\n",
    "        Name of the model's input tensor.\n",
    "    output_name : str\n",
    "        Name of the model's output tensor.\n",
    "    window : np.ndarray\n",
    "        Input window to validate and pass to the model.\n",
    "    verbose : bool\n",
    "        If True, print diagnostic messages.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys:\n",
    "        - \"ok\": bool\n",
    "        - \"error\": str or None\n",
    "        - \"prediction\": int or None\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Validation\n",
    "    is_valid = master_validate(window)\n",
    "\n",
    "    if not is_valid:\n",
    "        if verbose:\n",
    "            print(\"[secure_predict] Validation failed.\")\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"error\": \"validation_failed\",\n",
    "            \"prediction\": None,\n",
    "        }\n",
    "\n",
    "    # 2) Prepare input shape (ensure batch dimension)\n",
    "    try:\n",
    "        if window.ndim == 2:\n",
    "            # (W, C) -> (1, W, C)\n",
    "            window_batch = window[None, ...]\n",
    "        elif window.ndim == 3:\n",
    "            window_batch = window\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"[secure_predict] Unexpected input ndim:\", window.ndim)\n",
    "            return {\n",
    "                \"ok\": False,\n",
    "                \"error\": \"unexpected_ndim\",\n",
    "                \"prediction\": None,\n",
    "            }\n",
    "\n",
    "        # 3) Inference\n",
    "        outputs = session.run([output_name], {input_name: window_batch})\n",
    "        preds = outputs[0]\n",
    "\n",
    "        # Handle typical (1, num_classes) output\n",
    "        if preds.ndim == 2:\n",
    "            pred_idx = int(preds[0].argmax())\n",
    "        else:\n",
    "            pred_idx = int(preds.argmax())\n",
    "\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"error\": None,\n",
    "            \"prediction\": pred_idx,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(\"[secure_predict] Runtime error:\", repr(e))\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"error\": \"runtime_error\",\n",
    "            \"prediction\": None,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf0890-92f3-4a31-9fb9-2b902dae266d",
   "metadata": {},
   "source": [
    "### 3.3 - Testing the Safe Inference Wrapper\n",
    "\n",
    "To test `secure_predict`, we generate:\n",
    "\n",
    "1. A **synthetic valid window** with the correct shape and value range.\n",
    "2. The **invalid windows** created in Lab Step 2.5.\n",
    "\n",
    "The goal is to confirm:\n",
    "\n",
    "- `ok = True` and a valid `prediction` for the correct input.\n",
    "- `ok = False`, a meaningful `error`, and `prediction = None` for invalid inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40b789d-3e63-450b-96fb-66c130ec066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid window shape: (100, 7)\n",
      "Valid window dtype: float32\n",
      "Valid input result: {'ok': True, 'error': None, 'prediction': 1}\n"
     ]
    }
   ],
   "source": [
    "valid_window = np.random.uniform(\n",
    "    low=-1.0, high=1.0,\n",
    "    size=(window_length, num_channels)\n",
    ").astype(np.float32)\n",
    "\n",
    "print(\"Valid window shape:\", valid_window.shape)\n",
    "print(\"Valid window dtype:\", valid_window.dtype)\n",
    "\n",
    "result_valid = secure_predict(session, input_name, output_name, valid_window, verbose=True)\n",
    "print(\"Valid input result:\", result_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223e623-2289-4b3b-94aa-55401663c32d",
   "metadata": {},
   "source": [
    "### 3.4 - Testing Invalid Inputs (Robustness Verification)\n",
    "\n",
    "In this step, we verify that the safe inference wrapper correctly handles invalid inputs.  \n",
    "We test several failure modes: wrong shape, wrong dtype, out-of-range values, and NaNs.\n",
    "\n",
    "A valid system should reject each invalid input, return a clear error message, and avoid running inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "078770e8-3f59-4cc2-8185-befb7e1b982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running invalid input tests...\n",
      "\n",
      "--- Wrong shape ---\n",
      "[secure_predict] Validation failed.\n",
      "{'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "\n",
      "--- Wrong dtype ---\n",
      "[secure_predict] Validation failed.\n",
      "{'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "\n",
      "--- Out of range ---\n",
      "[secure_predict] Validation failed.\n",
      "{'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "\n",
      "--- Contains NaN ---\n",
      "[secure_predict] Validation failed.\n",
      "{'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# invalid_shape\n",
    "invalid_shape = np.zeros((1, window_length - 50, num_channels), dtype=np.float32)\n",
    "\n",
    "# invalid_dtype\n",
    "invalid_dtype = np.zeros((window_length, num_channels), dtype=np.float64)\n",
    "\n",
    "# invalid_range\n",
    "invalid_range = np.random.uniform(-10, 10, size=(window_length, num_channels)).astype(np.float32)\n",
    "\n",
    "# invalid_nan\n",
    "invalid_nan = np.random.uniform(-1, 1, size=(window_length, num_channels)).astype(np.float32)\n",
    "invalid_nan[0, 0] = np.nan\n",
    "\n",
    "\n",
    "tests = {\n",
    "    \"Wrong shape\": invalid_shape,\n",
    "    \"Wrong dtype\": invalid_dtype,\n",
    "    \"Out of range\": invalid_range,\n",
    "    \"Contains NaN\": invalid_nan,\n",
    "}\n",
    "\n",
    "print(\"Running invalid input tests...\\n\")\n",
    "\n",
    "for name, w in tests.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    result = secure_predict(session, input_name, output_name, w, verbose=True)\n",
    "    print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3221e16-4e1c-4802-8ba5-b154697276af",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- All invalid inputs should be rejected (`ok=False`).\n",
    "- The system should return a clear and meaningful error code.\n",
    "- No prediction should be produced for invalid inputs.\n",
    "- This validates that the wrapper enforces \"validation before inference,\" an essential principle for safety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace570cb-cd56-46b5-a09f-e8225433f398",
   "metadata": {},
   "source": [
    "## Lab Step 4 - Designing a Safe On-Device Inference API\n",
    "\n",
    "In the previous lab steps you implemented input validation and a secure inference wrapper.  \n",
    "We now look at how these components come together to form a **Safe On-Device Inference API**.\n",
    "\n",
    "This API acts as a protective boundary between the application and the model, ensuring that only valid inputs reach the inference engine and that failures are handled in a structured way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecf50e-cee8-439d-b608-0583e16beefd",
   "metadata": {},
   "source": [
    "### 4.1 - Why a Safe On-Device Inference API?\n",
    "\n",
    "Edge devices operate in environments where sensor data may be noisy, incomplete, or corrupted.  \n",
    "A safe API ensures that:\n",
    "\n",
    "- invalid inputs are rejected\n",
    "- inference is only executed when safe\n",
    "- applications receive predictable, structured responses\n",
    "\n",
    "This layer becomes the main entry point for calling the model inside an embedded or mobile system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb341c-2d16-43c5-a825-2beed8faf219",
   "metadata": {},
   "source": [
    "### 4.2 - API Structure and Contract\n",
    "\n",
    "A Safe Inference API defines a clear contract about:\n",
    "\n",
    "**1. What inputs are acceptable**\n",
    "- correct shape\n",
    "- correct data type\n",
    "- physically plausible values\n",
    "- no NaNs or infinities\n",
    "\n",
    "**2. What outputs look like**\n",
    "- structured dictionary (`ok`, `error`, `prediction`)\n",
    "- no silent failures\n",
    "\n",
    "**3. How errors are reported**\n",
    "- explicit, predictable error messages\n",
    "- no exceptions surfacing to the application\n",
    "\n",
    "This contract makes the system reliable and easy to integrate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d05f6-1888-40b3-acba-01f1095ebc62",
   "metadata": {},
   "source": [
    "### 4.3 - Handling Errors Consistently\n",
    "\n",
    "A safe API must behave predictably when inputs are invalid.\n",
    "\n",
    "Instead of:\n",
    "- raising exceptions, or\n",
    "- producing undefined results,\n",
    "\n",
    "the API returns a *clear error signal* that the application can act on.\n",
    "\n",
    "Consistent error handling is essential for robustness in real environments, where sensor disturbances or transmission issues may occur frequently.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eefb2b-ceb6-4615-a891-8b841bbd815e",
   "metadata": {},
   "source": [
    "### 4.4 - What a Safe Prediction Flow Looks Like\n",
    "\n",
    "A safe prediction flow follows these steps:\n",
    "\n",
    "1. **Collect input** (sensor window)\n",
    "2. **Validate the window** before inference\n",
    "3. **Decision:**\n",
    "   - If valid → run inference\n",
    "   - If invalid → return an error, no prediction\n",
    "4. **Produce structured output**\n",
    "\n",
    "This ensures the model is only called under safe, known conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681726d6-5621-4fab-bdfc-23db33063eb1",
   "metadata": {},
   "source": [
    "### 4.5 – How the Implementation Will Work (Conceptual)\n",
    "\n",
    "In the next sections of the notebook you will assemble:\n",
    "\n",
    "- the validation functions\n",
    "- the secure prediction wrapper\n",
    "- a clean API interface\n",
    "\n",
    "The API will expose a single call that applications can use safely, without knowing the internal details of validation or inference.\n",
    "\n",
    "This demonstrates how AI components can be packaged for reliable on-device use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef535a-d9ff-4192-a914-b45992ef33b2",
   "metadata": {},
   "source": [
    "## 5. Lab Step 5 - Implementing the Safe Inference API\n",
    "\n",
    "In this step, we turn the concepts from Section 7 into a working Python API.\n",
    "\n",
    "The API will:\n",
    "- validate inputs internally\n",
    "- handle errors consistently\n",
    "- run inference only when safe\n",
    "- return structured results with clear success/failure signals\n",
    "\n",
    "We implement the API as a Python class, supported by the validation functions and the secure prediction wrapper developed earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6e49f-e772-406c-99d0-225e4f790c48",
   "metadata": {},
   "source": [
    "### 5.1 – API Structure\n",
    "\n",
    "We implement the Safe Inference API as a Python class:\n",
    "\n",
    "- The class stores:\n",
    "  - the ONNX inference session\n",
    "  - the model's input/output tensor names\n",
    "  - expected shapes and data types\n",
    "\n",
    "- It exposes a single method:\n",
    "  `predict(window)`\n",
    "\n",
    "- Internally, it integrates:\n",
    "  - our master validator\n",
    "  - the secure prediction wrapper\n",
    "  - structured error communication\n",
    "\n",
    "This gives the application a clean and predictable interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "805cdc0f-caf4-4a06-aba0-118d76860c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeInferenceAPI:\n",
    "    \"\"\"\n",
    "    High-level API for safe, validated on-device inference.\n",
    "    Wraps validation + secure_predict into a simple interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, session, input_name, output_name, expected_shape, expected_dtype):\n",
    "        self.session = session\n",
    "        self.input_name = input_name\n",
    "        self.output_name = output_name\n",
    "        self.expected_shape = expected_shape\n",
    "        self.expected_dtype = expected_dtype\n",
    "\n",
    "    def predict(self, window, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Main public method: validate input and run secure prediction.\n",
    "        Returns a structured dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) Input validation\n",
    "        is_valid = validate_input(\n",
    "            window,\n",
    "            expected_shape=self.expected_shape,\n",
    "            expected_dtype=self.expected_dtype\n",
    "        )\n",
    "\n",
    "        if not is_valid:\n",
    "            if verbose:\n",
    "                print(\"[SafeInferenceAPI] Validation failed.\")\n",
    "            return {\n",
    "                \"ok\": False,\n",
    "                \"error\": \"validation_failed\",\n",
    "                \"prediction\": None\n",
    "            }\n",
    "\n",
    "        # 2) Secure inference\n",
    "        result = secure_predict(\n",
    "            self.session, \n",
    "            self.input_name, \n",
    "            self.output_name, \n",
    "            window, \n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe95f6-3f8e-4f69-8a11-4cfdfae5d847",
   "metadata": {},
   "source": [
    "### 5.2 - Creating an Instance of the API\n",
    "\n",
    "We now create an API object using:\n",
    "\n",
    "- our ONNX Runtime session\n",
    "- the model input/output tensor names\n",
    "- the expected input shape and dtype detected earlier\n",
    "\n",
    "This turns the model into a reusable, safe component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac3f9d7-fc93-456d-9ec2-084a0c0ce875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SafeInferenceAPI instance created.\n"
     ]
    }
   ],
   "source": [
    "api = SafeInferenceAPI(\n",
    "    session=session,\n",
    "    input_name=input_name,\n",
    "    output_name=output_name,\n",
    "    expected_shape=expected_shape,\n",
    "    expected_dtype=expected_dtype\n",
    ")\n",
    "\n",
    "print(\"SafeInferenceAPI instance created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd989f3-9c74-4398-b0ae-7a506ad962ad",
   "metadata": {},
   "source": [
    "### 5.3 – Test the API with a Valid Window\n",
    "\n",
    "We reuse a synthetic valid window to check the API’s behaviour.\n",
    "The API should return:\n",
    "\n",
    "- ok = True  \n",
    "- error = None  \n",
    "- prediction = <class index>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a660c767-172a-427e-b690-010a10848ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API result (valid input): {'ok': True, 'error': None, 'prediction': 1}\n"
     ]
    }
   ],
   "source": [
    "result_valid = api.predict(valid_window, verbose=True)\n",
    "print(\"API result (valid input):\", result_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953abd2-0cdb-49ac-ab72-2f885079dc9d",
   "metadata": {},
   "source": [
    "### 5.4 – Test the API with Invalid Inputs\n",
    "\n",
    "We now test the API with the intentionally malformed windows created earlier:\n",
    "\n",
    "- wrong shape  \n",
    "- wrong dtype  \n",
    "- out-of-range values  \n",
    "- values containing NaNs  \n",
    "\n",
    "All cases should:\n",
    "\n",
    "- fail validation  \n",
    "- return `ok = False`  \n",
    "- produce an appropriate error  \n",
    "- never run inference  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf0655c-cc81-44e6-82d8-d0193a6de059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Wrong shape ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "API result: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "\n",
      "--- Wrong dtype ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "API result: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "\n",
      "--- Out of range ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "API result: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "\n",
      "--- Contains NaN ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "API result: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n"
     ]
    }
   ],
   "source": [
    "tests = {\n",
    "    \"Wrong shape\": invalid_shape,\n",
    "    \"Wrong dtype\": invalid_dtype,\n",
    "    \"Out of range\": invalid_range,\n",
    "    \"Contains NaN\": invalid_nan,\n",
    "}\n",
    "\n",
    "for name, w in tests.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    result = api.predict(w, verbose=True)\n",
    "    print(\"API result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3571b0-9ea8-42f0-bcb3-957c56e51ea9",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "In your own words:\n",
    "\n",
    "- What protections does this API add compared to calling the model directly?\n",
    "- How does structured error reporting make application code more reliable?\n",
    "- Why is it important that the API exposes only *one* method (`predict`) for the application?\n",
    "\n",
    "Write a few sentences summarizing your insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3fdde-7d06-477c-a9fb-cbee719256d0",
   "metadata": {},
   "source": [
    "## 6. Lab Step 6 - Running the Complete Secure Pipeline\n",
    "\n",
    "In this final step, we run the entire secure prediction pipeline as a single system.\n",
    "This includes:\n",
    "\n",
    "- input window creation\n",
    "- validation\n",
    "- safe inference\n",
    "- structured output reporting\n",
    "\n",
    "This demonstrates how the Safe Inference API behaves under normal and abnormal operating conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1400987-b13c-4bcf-a800-fdbd3c45a978",
   "metadata": {},
   "source": [
    "### 6.1 - Running the Safe API on a Valid Input\n",
    "\n",
    "We start by running the API on a valid sensor window.\n",
    "The expected result is:\n",
    "\n",
    "- `ok = True`\n",
    "- `error = None`\n",
    "- a numerical prediction (class index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfbef093-0d8b-45ef-9a51-58ad95aff4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline test with a valid window...\n",
      "\n",
      "Pipeline result (valid input):\n",
      "{'ok': True, 'error': None, 'prediction': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Running pipeline test with a valid window...\\n\")\n",
    "\n",
    "result_valid_pipeline = api.predict(valid_window, verbose=True)\n",
    "print(\"Pipeline result (valid input):\")\n",
    "print(result_valid_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e6dee-4328-4a16-b080-c5a4326761df",
   "metadata": {},
   "source": [
    "### 6.2 - Running the Safe API on Invalid Inputs\n",
    "\n",
    "We now test the pipeline with several invalid windows:\n",
    "\n",
    "- wrong shape  \n",
    "- wrong dtype  \n",
    "- out-of-range values  \n",
    "- values containing NaNs  \n",
    "\n",
    "The pipeline should:\n",
    "\n",
    "- reject the input (`ok = False`)\n",
    "- produce a clear error message\n",
    "- avoid running inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c9e440b-1315-4381-8d3b-15336e49e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running pipeline tests with invalid inputs...\n",
      "\n",
      "--- Wrong shape ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "Pipeline result: {'ok': False, 'error': 'validation_failed', 'prediction': None} \n",
      "\n",
      "--- Wrong dtype ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "Pipeline result: {'ok': False, 'error': 'validation_failed', 'prediction': None} \n",
      "\n",
      "--- Out of range ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "Pipeline result: {'ok': False, 'error': 'validation_failed', 'prediction': None} \n",
      "\n",
      "--- Contains NaN ---\n",
      "[SafeInferenceAPI] Validation failed.\n",
      "Pipeline result: {'ok': False, 'error': 'validation_failed', 'prediction': None} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRunning pipeline tests with invalid inputs...\\n\")\n",
    "\n",
    "tests = {\n",
    "    \"Wrong shape\": invalid_shape,\n",
    "    \"Wrong dtype\": invalid_dtype,\n",
    "    \"Out of range\": invalid_range,\n",
    "    \"Contains NaN\": invalid_nan,\n",
    "}\n",
    "\n",
    "pipeline_results = {}\n",
    "\n",
    "for name, w in tests.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    result = api.predict(w, verbose=True)\n",
    "    pipeline_results[name] = result\n",
    "    print(\"Pipeline result:\", result, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416af7bd-8a5c-4aa0-b900-03f0390e5c60",
   "metadata": {},
   "source": [
    "### 6.3 - Summary of Pipeline Behaviour\n",
    "\n",
    "Below we produce a simple summary table comparing:\n",
    "\n",
    "- valid input behaviour  \n",
    "- invalid input behaviour  \n",
    "\n",
    "This helps visualize how the Safe Inference API responds under different conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96153700-6e04-4129-8067-92ccacf31f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pipeline Summary ---\n",
      "\n",
      "Valid Input: {'ok': True, 'error': None, 'prediction': 1}\n",
      "Wrong shape: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "Wrong dtype: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "Out of range: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n",
      "Contains NaN: {'ok': False, 'error': 'validation_failed', 'prediction': None}\n"
     ]
    }
   ],
   "source": [
    "summary = {\n",
    "    \"Valid Input\": result_valid_pipeline\n",
    "}\n",
    "\n",
    "summary.update(pipeline_results)\n",
    "\n",
    "print(\"\\n--- Pipeline Summary ---\\n\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f81ccf-c83f-4daf-baf2-fa9d8f7a6cce",
   "metadata": {},
   "source": [
    "### Final Reflection\n",
    "\n",
    "After completing this step:\n",
    "\n",
    "- How does the Safe Inference API protect the model?\n",
    "- What differences do you observe between valid and invalid inference paths?\n",
    "- How could this design be extended for real deployment (e.g., logging, rate limiting, metadata)?\n",
    "\n",
    "Write a short reflection summarizing your understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b3f4e1-03c2-4cac-b9eb-2f3eba801431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:unit21_5G]",
   "language": "python",
   "name": "conda-env-unit21_5G-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

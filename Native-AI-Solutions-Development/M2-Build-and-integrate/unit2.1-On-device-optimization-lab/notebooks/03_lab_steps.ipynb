{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aea572a",
   "metadata": {},
   "source": [
    "# Unit 2.1 - On-device optimization lab \n",
    "\n",
    "## Lab Step 1: Environment Setup + Basline Model Load\n",
    "\n",
    "In this step, we will set up the necessary environment for our native AI solutions development lab. We will install the required libraries, including ONNX Runtime, NumPy, Matplotlib, and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f86b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.19.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (0.5.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorflow==2.19.0) (0.31.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from keras>=3.5.0->tensorflow==2.19.0) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
      "Requirement already satisfied: onnx==1.16.2 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: onnxruntime==1.18.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (1.18.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from onnx==1.16.2) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from onnx==1.16.2) (3.20.3)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from onnxruntime==1.18.1) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from onnxruntime==1.18.1) (25.9.23)\n",
      "Requirement already satisfied: packaging in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from onnxruntime==1.18.1) (25.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from onnxruntime==1.18.1) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from coloredlogs->onnxruntime==1.18.1) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime==1.18.1) (3.5.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from sympy->onnxruntime==1.18.1) (1.3.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ikybe\\anaconda3\\envs\\unit21_5g\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Environmental setup. Install required libraries. It assumes a conda/anaconda environment with Python=3.10.X\n",
    "# In Windows, there is no tflite-runtime library (available on Linux, ARM, Android and other embedded systems)\n",
    "!pip install \"tensorflow==2.19.0\"\n",
    "!pip install \"onnx==1.16.2\" \"onnxruntime==1.18.1\"\n",
    "!pip install \"numpy==1.26.4\" matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644826b-141b-4638-93b1-9953e678d3c2",
   "metadata": {},
   "source": [
    "### 1.2 - Load the Preprocessed Dataset\n",
    "\n",
    "In this step, we load the preprocessed dataset generated from the UCA-EHAR recordings.  \n",
    "This dataset contains sliding windows of IMU + pressure sensor data for 8 human activities.\n",
    "\n",
    "Make sure the file **`uca_ehar_preprocessed_win100_step50.npz`** is placed in the same folder as this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cee87d-44bc-4cf7-92fb-98a4fb0c15b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Training data shape: (8241, 100, 7)\n",
      "Validation data shape: (1766, 100, 7)\n",
      "Test data shape: (1767, 100, 7)\n",
      "\n",
      "Activity classes: ['STANDING' 'SITTING' 'WALKING' 'WALKING_UPSTAIRS' 'WALKING_DOWNSTAIRS'\n",
      " 'RUNNING' 'LYING' 'DRINKING']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "DATASET_PATH = os.path.join(\"..\", \"data\", \"uca_ehar_preprocessed_win100_step50.npz\")\n",
    "\n",
    "# Load dataset\n",
    "data = np.load(DATASET_PATH, allow_pickle=True)\n",
    "\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "X_val = data[\"X_val\"]\n",
    "y_val = data[\"y_val\"]\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "class_names = data[\"class_names\"]\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"\\nActivity classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32a65c8-112a-4294-b48c-4aa5d0736b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window length: 100\n",
      "Number of sensor channels: 7\n",
      "Labels present: [0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "# Code cell (quick sanity checks — optional but recommended)\n",
    "# Check window size and signal count\n",
    "print(\"Window length:\", X_train.shape[1])\n",
    "print(\"Number of sensor channels:\", X_train.shape[2])\n",
    "\n",
    "# Check unique labels\n",
    "unique_labels = np.unique(y_train)\n",
    "print(\"Labels present:\", unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15bd43-41d2-4491-bde7-499332eeae1c",
   "metadata": {},
   "source": [
    "### 1.3 - Load the Baseline HAR Model\n",
    "\n",
    "In this step you will load the *baseline* Human Activity Recognition (HAR) model.\n",
    "\n",
    "- The model is stored in ONNX format: `har_baseline.onnx`\n",
    "- It was trained beforehand on the same UCA-EHAR dataset you are using\n",
    "- In this lab we will only use **CPU** inference via **ONNX Runtime**\n",
    "\n",
    "After loading the model, we will inspect:\n",
    "- its input and output shapes\n",
    "- its file size on disk (model footprint)\n",
    "\n",
    "Make sure the file:\n",
    "\n",
    "`models/har_baseline.onnx`\n",
    "\n",
    "exists in the `models/` folder of your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf47746-9b3c-449b-93d0-6438531809c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline HAR model from: ..\\models\\har_baseline.onnx\n",
      "Model loaded successfully ✅\n",
      "Input name : input\n",
      "Input shape: ['unk__121', 100, 7]\n",
      "Output name: dense_1\n",
      "Output shape: ['unk__122', 8]\n",
      "Number of classes: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Path to the baseline ONNX model\n",
    "MODEL_PATH = os.path.join(\"..\", \"models\", \"har_baseline.onnx\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Baseline model not found at: {MODEL_PATH}\")\n",
    "\n",
    "print(\"Loading baseline HAR model from:\", MODEL_PATH)\n",
    "\n",
    "# Create ONNX Runtime inference session (CPU-only)\n",
    "session = ort.InferenceSession(MODEL_PATH, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Get input / output names and shapes\n",
    "input_tensor  = session.get_inputs()[0]\n",
    "output_tensor = session.get_outputs()[0]\n",
    "\n",
    "input_name  = input_tensor.name\n",
    "output_name = output_tensor.name\n",
    "\n",
    "print(\"Model loaded successfully ✅\")\n",
    "print(\"Input name :\", input_name)\n",
    "print(\"Input shape:\", input_tensor.shape)\n",
    "print(\"Output name:\", output_name)\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "\n",
    "# Optional: show number of classes if class_names was loaded earlier from the dataset\n",
    "try:\n",
    "    print(\"Number of classes:\", len(class_names))\n",
    "except NameError:\n",
    "    print(\"Hint: run the dataset loading cell first to see class names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9232735-92e9-4f87-a6cc-bdcd848536bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model size: 327.19 KB\n"
     ]
    }
   ],
   "source": [
    "# Inspecting model size on disk\n",
    "\n",
    "size_kb = os.path.getsize(MODEL_PATH) / 1024\n",
    "print(f\"Baseline model size: {size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1d8dc-019b-4d88-b19c-ece126a1a1b6",
   "metadata": {},
   "source": [
    "## Lab Step 2: Measure Baseline KPIs\n",
    "\n",
    "The first technical task is to evaluate the baseline model performance before optimization.\n",
    "We measure:\n",
    "- Latency (inference time)\n",
    "- Model size\n",
    "- Initial accuracy\n",
    "- Energy proxy (optional)\n",
    "These measurements form the baseline against which we compare improvements after quantization and pruning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9bc7cc-4f61-4797-89b5-351996b51b54",
   "metadata": {},
   "source": [
    "### 2.1 - Prepare Test Data for Benchmarking\n",
    "\n",
    "In this step you will prepare the data used to benchmark the baseline model.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Use the **test split** of the preprocessed UCA-EHAR dataset  \n",
    "- Create a reference array `X_bench, y_bench` for all KPI measurements\n",
    "\n",
    "All latency and accuracy measurements in this lab will be based on this benchmark set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc8ef44-b0cb-45a1-8016-51e680a65d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (8241, 100, 7)\n",
      "Val set shape  : (1766, 100, 7)\n",
      "Test set shape : (1767, 100, 7)\n",
      "train_mean shape: (1, 1, 7)\n",
      "train_std  shape: (1, 1, 7)\n",
      "Benchmark set shape: (1767, 100, 7)\n",
      "Benchmark labels shape: (1767,)\n",
      "Normalized X_bench mean: -0.11365111 std: 0.98807645\n"
     ]
    }
   ],
   "source": [
    "# Assuming (from part1) your dataset-loading cell already created X_train, X_val, X_test, y_train, y_val, y_test, class_names.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We assume X_train, X_val, X_test, y_train, y_val, y_test, class_names\n",
    "# have already been loaded from the NPZ file earlier in the notebook.\n",
    "\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Val set shape  :\", X_val.shape)\n",
    "print(\"Test set shape :\", X_test.shape)\n",
    "\n",
    "# --- Recompute training normalization (same as in teacher notebook) ---\n",
    "\n",
    "# Compute mean and std ONLY on the training set\n",
    "train_mean = X_train.mean(axis=(0, 1), keepdims=True)   # shape: (1, 1, num_channels)\n",
    "train_std  = X_train.std(axis=(0, 1), keepdims=True)\n",
    "train_std[train_std == 0] = 1.0   # safety\n",
    "\n",
    "print(\"train_mean shape:\", train_mean.shape)\n",
    "print(\"train_std  shape:\", train_std.shape)\n",
    "\n",
    "# Apply normalization to all splits\n",
    "X_train_n = (X_train - train_mean) / train_std\n",
    "X_val_n   = (X_val   - train_mean) / train_std\n",
    "X_test_n  = (X_test  - train_mean) / train_std\n",
    "\n",
    "# Benchmark set = normalized test set\n",
    "X_bench = X_test_n.astype(np.float32)\n",
    "y_bench = y_test.copy()\n",
    "\n",
    "print(\"Benchmark set shape:\", X_bench.shape)\n",
    "print(\"Benchmark labels shape:\", y_bench.shape)\n",
    "print(\"Normalized X_bench mean:\", X_bench.mean(), \"std:\", X_bench.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39937ffc-ded1-4e4c-8e79-e3519589380b",
   "metadata": {},
   "source": [
    "### 2.2 - Latency Measurement (CPU-only)\n",
    "\n",
    "First, we measure **inference latency** for a single window.\n",
    "\n",
    "- We will use ONNX Runtime on CPU only\n",
    "- We repeat inference several times and average the result\n",
    "- Latency is reported in **milliseconds per inference**\n",
    "\n",
    "Remember: the scenario target from Unit 1.1 is **< 20 ms** per inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26fc783d-527b-49ff-93d2-ee2cac1c8001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time (baseline): 0.17 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_latency(sample: np.ndarray, runs: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Measure average inference time (ms) for a single sample.\n",
    "\n",
    "    Args:\n",
    "        sample: Input window with shape (1, window_size, num_channels)\n",
    "        runs: Number of repetitions for averaging\n",
    "\n",
    "    Returns:\n",
    "        Average latency in milliseconds.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        _ = session.run([output_name], {input_name: sample})\n",
    "    end = time.time()\n",
    "    return (end - start) * 1000.0 / runs  # ms per inference\n",
    "\n",
    "\n",
    "# Take the first benchmark window\n",
    "sample = X_bench[0:1]   # shape: (1, 100, 7) for this dataset\n",
    "\n",
    "latency_ms = measure_latency(sample, runs=100)\n",
    "print(f\"Inference time (baseline): {latency_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc0375-1df8-42a1-8cf3-169af88165c8",
   "metadata": {},
   "source": [
    "### 2.3 - Accuracy Measurement\n",
    "\n",
    "Next, we measure **classification accuracy** of the baseline model.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Run the model on all benchmark windows  \n",
    "2. Collect the predicted class index for each window  \n",
    "3. Compare predictions with the true labels\n",
    "\n",
    "Accuracy is reported as a percentage. For HAR, typical baseline accuracy is\n",
    "between **90–95%**, depending on model size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1cdd7a-658b-482c-a8fc-9d9b8407d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on benchmark set: 92.98%\n"
     ]
    }
   ],
   "source": [
    "def predict(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run inference on a batch of windows and return predicted class indices.\n",
    "\n",
    "    Args:\n",
    "        X: Array of shape (num_windows, window_size, num_channels)\n",
    "\n",
    "    Returns:\n",
    "        Array of shape (num_windows,) with predicted class indices.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for window in X:\n",
    "        # Add batch dimension: (W, C) -> (1, W, C)\n",
    "        out = session.run([output_name], {input_name: window[None, ...]})[0]\n",
    "        preds.append(out.argmax())\n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "y_pred = predict(X_bench)\n",
    "accuracy = (y_pred == y_bench).mean() * 100.0\n",
    "\n",
    "print(f\"Baseline accuracy on benchmark set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764b24d-8b69-4549-a976-22616a40020f",
   "metadata": {},
   "source": [
    "### 2.4 - Optional Energy Proxy (MACs / FLOPs)\n",
    "\n",
    "As an optional step, we can estimate an **energy proxy** using MACs/FLOPs\n",
    "(Multiply–Accumulate operations). This is not a direct energy measurement, but\n",
    "gives a sense of how computationally expensive the model is.\n",
    "\n",
    "If the helper script is available, run the cell below. Otherwise, you can skip\n",
    "this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8bc411-3794-47c7-9e2b-358822e05149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MACs for baseline model: 17408\n"
     ]
    }
   ],
   "source": [
    "# Note: The MACs value below is an approximate proxy based on how the model is represented in ONNX. It may not count every operation exactly, but it is good\n",
    "# enough to compare **relative** complexity between different model versions.\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory (where \"utils\" lives) to Python path\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from utils.metrics2 import estimate_macs  # now this should work\n",
    "\n",
    "\n",
    "macs = None\n",
    "\n",
    "try:    \n",
    "    model_path_rel = os.path.join(\"..\", \"models\", \"har_baseline.onnx\")\n",
    "    macs = estimate_macs(model_path_rel)\n",
    "    print(\"Estimated MACs for baseline model:\", macs)\n",
    "except Exception as e:\n",
    "    print(\"MACs estimation is not available. Reason:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8b8e7-9820-4997-bfca-7fa6094d4d5c",
   "metadata": {},
   "source": [
    "### 2.5 – Baseline KPI Table\n",
    "\n",
    "Use the values you just measured to fill in this table in your notes or\n",
    "Optimization Report:\n",
    "\n",
    "| Metric        | Baseline Value         | Scenario Target      | Pass? (✓ / ✗) |\n",
    "|--------------|------------------------|----------------------|---------------|\n",
    "| Latency (ms) | `...`                  | `< 20 ms`            | `...`         |\n",
    "| Size (KB/MB) | `...`                  | `< 1 MB`             | `...`         |\n",
    "| Accuracy (%) | `...`                  | `≥ 90%` (≥ 88% min)  | `...`         |\n",
    "| MACs         | `...` (if measured)    | “As low as possible” | `—`           |\n",
    "\n",
    "These **baseline KPIs** will be your reference when you apply quantization and\n",
    "pruning in the next steps. After optimization, you will recompute the same\n",
    "metrics and compare them with this table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df19f76-a274-47e6-ab6c-87d7ee10b0e4",
   "metadata": {},
   "source": [
    "## Lab Step 3:  Apply Quantization\n",
    "\n",
    "### 3.1 - Quantization Workflow\n",
    "\n",
    "In this step you will apply **post-training quantization** to the baseline HAR model.\n",
    "\n",
    "We will use **ONNX Runtime dynamic quantization** to:\n",
    "\n",
    "- convert the model weights from `float32` to `int8`\n",
    "- reduce model size on disk\n",
    "- improve CPU inference latency\n",
    "\n",
    "For CPU-only devices like our smart safety glasses, dynamic quantization is a\n",
    "simple but powerful optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8ba49f-2465-42b8-955e-cd126b05cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model path : ..\\models\\har_baseline.onnx\n",
      "Quantized model path: ..\\models\\har_quantized.onnx\n",
      "Quantization complete. Quantized model saved to: ..\\models\\har_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Paths for baseline and quantized models\n",
    "BASELINE_MODEL_PATH = os.path.join(\"..\", \"models\", \"har_baseline.onnx\")\n",
    "QUANT_MODEL_PATH    = os.path.join(\"..\", \"models\", \"har_quantized.onnx\")\n",
    "\n",
    "print(\"Baseline model path :\", BASELINE_MODEL_PATH)\n",
    "print(\"Quantized model path:\", QUANT_MODEL_PATH)\n",
    "\n",
    "# Apply dynamic quantization ONLY to MatMul/Gemm (dense layers)\n",
    "quantize_dynamic(\n",
    "    model_input=BASELINE_MODEL_PATH,\n",
    "    model_output=QUANT_MODEL_PATH,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    op_types_to_quantize=[\"MatMul\", \"Gemm\"],  # do NOT quantize Conv\n",
    ")\n",
    "\n",
    "print(\"Quantization complete. Quantized model saved to:\", QUANT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fc630-a783-4563-a5b6-72aece5d5f9b",
   "metadata": {},
   "source": [
    "### 3.2 - Compare Model Sizes\n",
    "\n",
    "Then, compare the file size of the **baseline** model and the **quantized**\n",
    "model on disk.\n",
    "\n",
    "This is a direct measure of how much storage we save by moving from float32\n",
    "weights to int8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48d0a491-9b47-4fd5-95a0-e914473a2065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model size : 327.15 KB\n",
      "Quantized model size: 281.05 KB\n",
      "Size reduction      : 14.1%\n"
     ]
    }
   ],
   "source": [
    "size_baseline_kb = os.path.getsize(BASELINE_MODEL_PATH) / 1024\n",
    "size_quant_kb    = os.path.getsize(QUANT_MODEL_PATH) / 1024\n",
    "\n",
    "print(f\"Baseline model size : {size_baseline_kb:.2f} KB\")\n",
    "print(f\"Quantized model size: {size_quant_kb:.2f} KB\")\n",
    "print(f\"Size reduction      : {100 * (1 - size_quant_kb / size_baseline_kb):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf0890-92f3-4a31-9fb9-2b902dae266d",
   "metadata": {},
   "source": [
    "### 3.3 - Load the Quantized Model\n",
    "\n",
    "Next, load the quantized ONNX model into a new ONNX Runtime session and inspect\n",
    "its input and output shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f40b789d-3e63-450b-96fb-66c130ec066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model loaded successfully ✅\n",
      "Input name : input\n",
      "Input shape: ['unk__121', 100, 7]\n",
      "Output name: dense_1\n",
      "Output shape: ['unk__122', 8]\n"
     ]
    }
   ],
   "source": [
    "# Create a new session for the quantized model\n",
    "session_q = ort.InferenceSession(\n",
    "    QUANT_MODEL_PATH,\n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "q_input_tensor  = session_q.get_inputs()[0]\n",
    "q_output_tensor = session_q.get_outputs()[0]\n",
    "\n",
    "q_input_name  = q_input_tensor.name\n",
    "q_output_name = q_output_tensor.name\n",
    "\n",
    "print(\"Quantized model loaded successfully ✅\")\n",
    "print(\"Input name :\", q_input_name)\n",
    "print(\"Input shape:\", q_input_tensor.shape)\n",
    "print(\"Output name:\", q_output_name)\n",
    "print(\"Output shape:\", q_output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23a4af-3841-4ec2-b7d2-02ff1de21687",
   "metadata": {},
   "source": [
    "### 3.4 - Latency After Quantization\n",
    "\n",
    "Now we measure inference latency again, this time using the **quantized**\n",
    "model.\n",
    "\n",
    "To make the comparison fair:\n",
    "\n",
    "- we use the **same benchmark sample** as in Step 2\n",
    "- we use the **same timing method** (average over many runs)\n",
    "\n",
    "This allows us to see how much latency improvement we get from quantization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c2c20a0-6af3-4410-8543-78e7543bcd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline latency  : 0.152 ms\n",
      "Quantized latency : 0.136 ms\n",
      "Relative speed-up : 10.6%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_latency_session(sess, in_name, out_name, samples: np.ndarray, runs: int = 50) -> float:\n",
    "    \"\"\"\n",
    "    Measure average inference time (ms) over a batch of samples.\n",
    "    \"\"\"\n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        _ = sess.run([out_name], {in_name: samples[0:1]})\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        for window in samples:\n",
    "            _ = sess.run([out_name], {in_name: window[None, ...]})\n",
    "    end = time.time()\n",
    "\n",
    "    num_calls = runs * len(samples)\n",
    "    return (end - start) * 1000.0 / num_calls\n",
    "\n",
    "\n",
    "# Use the first 100 benchmark windows\n",
    "samples = X_bench[:100]\n",
    "\n",
    "latency_baseline_ms = measure_latency_session(session,  input_name,  output_name,  samples, runs=20)\n",
    "latency_quant_ms    = measure_latency_session(session_q, q_input_name, q_output_name, samples, runs=20)\n",
    "\n",
    "print(f\"Baseline latency  : {latency_baseline_ms:.3f} ms\")\n",
    "print(f\"Quantized latency : {latency_quant_ms:.3f} ms\")\n",
    "\n",
    "if latency_quant_ms > 0:\n",
    "    speedup = 100 * (1 - latency_quant_ms / latency_baseline_ms)\n",
    "    print(f\"Relative speed-up : {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac034ac-5849-4146-8ec8-ef277bce028b",
   "metadata": {},
   "source": [
    "### 3.5 – Accuracy After Quantization\n",
    "\n",
    "Now we check whether quantization has changed the **classification accuracy**.\n",
    "\n",
    "We reuse the same benchmark set (`X_bench`, `y_bench`) and compare:\n",
    "\n",
    "- baseline model accuracy\n",
    "- quantized model accuracy\n",
    "\n",
    "Our goal is for the accuracy drop to be small (ideally within 1–2 percentage\n",
    "points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0facf4f-0361-4e11-901d-73ff7bf9ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy  : 92.98%\n",
      "Quantized accuracy : 93.04%\n",
      "Accuracy change    : 0.06 percentage points\n"
     ]
    }
   ],
   "source": [
    "def predict_with_session(sess, in_name, out_name, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run inference on a batch of windows with a given ONNX Runtime session.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for window in X:\n",
    "        out = sess.run([out_name], {in_name: window[None, ...]})[0]\n",
    "        preds.append(out.argmax())\n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "# Accuracy for baseline model\n",
    "y_pred_baseline = predict_with_session(session, input_name, output_name, X_bench)\n",
    "accuracy_baseline = (y_pred_baseline == y_bench).mean() * 100.0\n",
    "\n",
    "# Accuracy for quantized model\n",
    "y_pred_quant = predict_with_session(session_q, q_input_name, q_output_name, X_bench)\n",
    "accuracy_quant = (y_pred_quant == y_bench).mean() * 100.0\n",
    "\n",
    "print(f\"Baseline accuracy  : {accuracy_baseline:.2f}%\")\n",
    "print(f\"Quantized accuracy : {accuracy_quant:.2f}%\")\n",
    "print(f\"Accuracy change    : {accuracy_quant - accuracy_baseline:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923caf59-eaaf-4eb2-91c0-814a261d58fe",
   "metadata": {},
   "source": [
    "## Lab Step 4:  Apply Quantization\n",
    "\n",
    "### 4.1 - Create a pruned version of the baseline model (global magnitude pruning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61b93f6e-7f97-48b2-b7f9-466ef486a07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model path: ..\\models\\har_baseline.onnx\n",
      "Pruned model path   : ..\\models\\har_pruned.onnx\n",
      "Found 14 weight tensors to prune.\n",
      "Global magnitude pruning completed ✅\n",
      "Target prune ratio   : 30.0%\n",
      "Actual pruned weights: 24557/81856 (30.0% set to zero)\n",
      "Saved pruned model to: ..\\models\\har_pruned.onnx\n"
     ]
    }
   ],
   "source": [
    "# 4.1 – Create a pruned version of the baseline model\n",
    "# (global magnitude-based pruning of float weight tensors)\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "\n",
    "# Paths\n",
    "MODELS_DIR = Path(\"..\") / \"models\"\n",
    "BASELINE_MODEL_PATH = MODELS_DIR / \"har_baseline.onnx\"\n",
    "PRUNED_MODEL_PATH   = MODELS_DIR / \"har_pruned.onnx\"\n",
    "\n",
    "print(\"Baseline model path:\", BASELINE_MODEL_PATH)\n",
    "print(\"Pruned model path   :\", PRUNED_MODEL_PATH)\n",
    "\n",
    "# Fraction of smallest-magnitude weights to set to zero (e.g. 0.3 = 30%)\n",
    "PRUNE_RATIO = 0.30\n",
    "\n",
    "# Load baseline ONNX model\n",
    "model = onnx.load(BASELINE_MODEL_PATH)\n",
    "\n",
    "# Collect candidate weight tensors: float, at least 2D (Conv / Dense kernels)\n",
    "weight_tensors = []\n",
    "for initializer in model.graph.initializer:\n",
    "    if initializer.data_type == onnx.TensorProto.FLOAT and len(initializer.dims) >= 2:\n",
    "        W = numpy_helper.to_array(initializer)\n",
    "        weight_tensors.append((initializer, W))\n",
    "\n",
    "print(f\"Found {len(weight_tensors)} weight tensors to prune.\")\n",
    "\n",
    "# Build a global list of all absolute weight values\n",
    "all_weights = np.concatenate([np.abs(W).ravel() for _, W in weight_tensors])\n",
    "total_params = all_weights.size\n",
    "\n",
    "# Global threshold so that ~PRUNE_RATIO of weights will be set to zero\n",
    "threshold = np.quantile(all_weights, PRUNE_RATIO)\n",
    "\n",
    "num_pruned = 0\n",
    "\n",
    "for initializer, W in weight_tensors:\n",
    "    mask = np.abs(W) < threshold\n",
    "    num_pruned += int(mask.sum())\n",
    "    W_pruned = W.copy()\n",
    "    W_pruned[mask] = 0.0\n",
    "    # Write pruned weights back into the ONNX initializer\n",
    "    initializer.CopyFrom(numpy_helper.from_array(W_pruned, initializer.name))\n",
    "\n",
    "sparsity = 100.0 * num_pruned / total_params\n",
    "\n",
    "# Save pruned model\n",
    "onnx.save(model, PRUNED_MODEL_PATH)\n",
    "\n",
    "print(\"Global magnitude pruning completed ✅\")\n",
    "print(f\"Target prune ratio   : {PRUNE_RATIO*100:.1f}%\")\n",
    "print(f\"Actual pruned weights: {num_pruned}/{total_params} \"\n",
    "      f\"({sparsity:.1f}% set to zero)\")\n",
    "print(\"Saved pruned model to:\", PRUNED_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b81e92-9df4-40ac-81fa-7c79fd110d11",
   "metadata": {},
   "source": [
    "### 4.2 - Load the pruned model and inspect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b128e62e-7575-4cf7-ac78-e6be784d7bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loaded successfully ✅\n",
      "Input name : input\n",
      "Input shape: ['unk__121', 100, 7]\n",
      "Output name: dense_1\n",
      "Output shape: ['unk__122', 8]\n"
     ]
    }
   ],
   "source": [
    "PRUNED_MODEL_PATH = os.path.join(\"..\", \"models\", \"har_pruned.onnx\")\n",
    "\n",
    "session_p = ort.InferenceSession(\n",
    "    PRUNED_MODEL_PATH,\n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "p_input_tensor  = session_p.get_inputs()[0]\n",
    "p_output_tensor = session_p.get_outputs()[0]\n",
    "\n",
    "p_input_name  = p_input_tensor.name\n",
    "p_output_name = p_output_tensor.name\n",
    "\n",
    "print(\"Pruned model loaded successfully ✅\")\n",
    "print(\"Input name :\", p_input_name)\n",
    "print(\"Input shape:\", p_input_tensor.shape)\n",
    "print(\"Output name:\", p_output_name)\n",
    "print(\"Output shape:\", p_output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b80aac-74dd-4b19-9083-8953b949e9c4",
   "metadata": {},
   "source": [
    "### 4.3 - Compare model sizes (baseline vs pruned vs quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2334892b-d67a-4f48-b145-21cf6e6d09a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model size : 327.15 KB\n",
      "Quantized model size: 281.05 KB\n",
      "Pruned model size   : 327.15 KB\n",
      "Pruned vs baseline  : 0.0% smaller\n",
      "Pruned vs quantized : -16.4% smaller\n"
     ]
    }
   ],
   "source": [
    "BASELINE_MODEL_PATH = os.path.join(\"..\", \"models\", \"har_baseline.onnx\")\n",
    "QUANT_MODEL_PATH    = os.path.join(\"..\", \"models\", \"har_quantized.onnx\")\n",
    "PRUNED_MODEL_PATH   = os.path.join(\"..\", \"models\", \"har_pruned.onnx\")\n",
    "\n",
    "size_baseline_kb = os.path.getsize(BASELINE_MODEL_PATH) / 1024\n",
    "size_quant_kb    = os.path.getsize(QUANT_MODEL_PATH) / 1024\n",
    "size_pruned_kb   = os.path.getsize(PRUNED_MODEL_PATH) / 1024\n",
    "\n",
    "print(f\"Baseline model size : {size_baseline_kb:.2f} KB\")\n",
    "print(f\"Quantized model size: {size_quant_kb:.2f} KB\")\n",
    "print(f\"Pruned model size   : {size_pruned_kb:.2f} KB\")\n",
    "\n",
    "print(f\"Pruned vs baseline  : {100 * (1 - size_pruned_kb / size_baseline_kb):.1f}% smaller\")\n",
    "print(f\"Pruned vs quantized : {100 * (1 - size_pruned_kb / size_quant_kb):.1f}% smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e83154-75f4-470c-b64b-bc9901bf1757",
   "metadata": {},
   "source": [
    "### 4.4 - Accuracy of the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e0c8612-bfc2-48cb-af1d-880734ac846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy : 92.98%\n",
      "Pruned accuracy   : 89.76%\n",
      "Accuracy change   : -3.23 percentage points\n"
     ]
    }
   ],
   "source": [
    "# Reuse the helper we defined before\n",
    "def predict_with_session(sess, in_name, out_name, X: np.ndarray) -> np.ndarray:\n",
    "    preds = []\n",
    "    for window in X:\n",
    "        out = sess.run([out_name], {in_name: window[None, ...]})[0]\n",
    "        preds.append(out.argmax())\n",
    "    return np.array(preds)\n",
    "\n",
    "# Baseline accuracy (recomputed for clarity)\n",
    "y_pred_baseline = predict_with_session(session, input_name, output_name, X_bench)\n",
    "acc_baseline = (y_pred_baseline == y_bench).mean() * 100.0\n",
    "\n",
    "# Pruned model accuracy\n",
    "y_pred_pruned = predict_with_session(session_p, p_input_name, p_output_name, X_bench)\n",
    "acc_pruned = (y_pred_pruned == y_bench).mean() * 100.0\n",
    "\n",
    "print(f\"Baseline accuracy : {acc_baseline:.2f}%\")\n",
    "print(f\"Pruned accuracy   : {acc_pruned:.2f}%\")\n",
    "print(f\"Accuracy change   : {acc_pruned - acc_baseline:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd74c1-833d-49ca-b5f3-2cd84929773e",
   "metadata": {},
   "source": [
    "### 4.5 - Latency of the pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8518897-a15d-4547-b85c-571a29679676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline latency : 0.170 ms\n",
      "Quantized latency: 0.138 ms\n",
      "Pruned latency   : 0.139 ms\n",
      "Pruned vs baseline speed-up: 17.9%\n",
      "Pruned vs quantized speed-up: -0.5%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure_latency_session(sess, in_name, out_name, samples: np.ndarray, runs: int = 20) -> float:\n",
    "    \"\"\"\n",
    "    Measure average inference time (ms) over a batch of samples.\n",
    "    \"\"\"\n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        _ = sess.run([out_name], {in_name: samples[0:1]})\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        for window in samples:\n",
    "            _ = sess.run([out_name], {in_name: window[None, ...]})\n",
    "    end = time.time()\n",
    "\n",
    "    num_calls = runs * len(samples)\n",
    "    return (end - start) * 1000.0 / num_calls  # ms per inference\n",
    "\n",
    "\n",
    "# Use the first 100 benchmark windows\n",
    "samples = X_bench[:100]\n",
    "\n",
    "lat_baseline = measure_latency_session(session,  input_name,  output_name,  samples, runs=20)\n",
    "lat_quant    = measure_latency_session(session_q, q_input_name, q_output_name, samples, runs=20)\n",
    "lat_pruned   = measure_latency_session(session_p, p_input_name, p_output_name, samples, runs=20)\n",
    "\n",
    "print(f\"Baseline latency : {lat_baseline:.3f} ms\")\n",
    "print(f\"Quantized latency: {lat_quant:.3f} ms\")\n",
    "print(f\"Pruned latency   : {lat_pruned:.3f} ms\")\n",
    "\n",
    "print(f\"Pruned vs baseline speed-up: {100 * (1 - lat_pruned / lat_baseline):.1f}%\")\n",
    "print(f\"Pruned vs quantized speed-up: {100 * (1 - lat_pruned / lat_quant):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41f5c2-b72c-4a78-b46d-8ed78c3cb400",
   "metadata": {},
   "source": [
    "### 4.6 - Optional: MACs / energy proxy for pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de02c2b5-59b9-49fa-a40d-b9347ba9c590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MACs estimate: 17408\n",
      "Pruned MACs estimate  : 17408\n",
      "Reduction: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory (where \"utils\" lives) to Python path\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from utils.metrics2 import estimate_macs  # now this should work\n",
    "\n",
    "base_path   = os.path.join(\"..\", \"models\", \"har_baseline.onnx\")\n",
    "pruned_path = os.path.join(\"..\", \"models\", \"har_pruned.onnx\")\n",
    "\n",
    "macs_base   = estimate_macs(base_path)\n",
    "macs_pruned = estimate_macs(pruned_path)\n",
    "\n",
    "print(\"Baseline MACs estimate:\", macs_base)\n",
    "print(\"Pruned MACs estimate  :\", macs_pruned)\n",
    "\n",
    "if macs_base > 0:\n",
    "    print(f\"Reduction: {100 * (1 - macs_pruned / macs_base):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a2548-28a7-4740-829d-4d3fc82ec0b7",
   "metadata": {},
   "source": [
    "## LAB STEP 5: Combine Pruning + Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8a579-4816-4085-90e3-27646c8868c8",
   "metadata": {},
   "source": [
    "### 5.1 - Create the Pruned + Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c94625d-b1ad-4a34-9b9c-74d5f028ddf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  model: ..\\models\\har_pruned.onnx\n",
      "Output model: ..\\models\\har_pruned_quantized.onnx\n",
      "Pruned + Quantized model saved successfully ✅\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os\n",
    "\n",
    "PRUNED_MODEL_PATH = os.path.join(\"..\", \"models\", \"har_pruned.onnx\")\n",
    "PRUNED_QUANT_MODEL_PATH = os.path.join(\"..\", \"models\", \"har_pruned_quantized.onnx\")\n",
    "\n",
    "print(\"Input  model:\", PRUNED_MODEL_PATH)\n",
    "print(\"Output model:\", PRUNED_QUANT_MODEL_PATH)\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=PRUNED_MODEL_PATH,\n",
    "    model_output=PRUNED_QUANT_MODEL_PATH,\n",
    "    op_types_to_quantize=[\"MatMul\", \"Gemm\"],\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "\n",
    "print(\"Pruned + Quantized model saved successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc918e4a-33e9-4d99-a876-6dcf1731b1fc",
   "metadata": {},
   "source": [
    "### 5.2 - Load the Pruned + Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cb0b3d0-9c21-4295-b01d-55c973f8b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned + quantized model loaded successfully ✅\n",
      "Input name : input\n",
      "Input shape: ['unk__121', 100, 7]\n",
      "Output name: dense_1\n",
      "Output shape: ['unk__122', 8]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import os\n",
    "\n",
    "PRUNED_QUANT_MODEL_PATH = os.path.join(\"..\", \"models\", \"har_pruned_quantized.onnx\")\n",
    "\n",
    "session_pq = ort.InferenceSession(\n",
    "    PRUNED_QUANT_MODEL_PATH,\n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "pq_input_tensor  = session_pq.get_inputs()[0]\n",
    "pq_output_tensor = session_pq.get_outputs()[0]\n",
    "\n",
    "pq_input_name  = pq_input_tensor.name\n",
    "pq_output_name = pq_output_tensor.name\n",
    "\n",
    "print(\"Pruned + quantized model loaded successfully ✅\")\n",
    "print(\"Input name :\", pq_input_name)\n",
    "print(\"Input shape:\", pq_input_tensor.shape)\n",
    "print(\"Output name:\", pq_output_name)\n",
    "print(\"Output shape:\", pq_output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0deca2-bece-49b9-ac2d-99e46f435b9b",
   "metadata": {},
   "source": [
    "### 5.3 - Compare model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5f64ff4-3a27-4eb9-a632-d8c5fd9d7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model size        : 327.15 KB\n",
      "Quantized model size       : 281.05 KB\n",
      "Pruned model size          : 327.15 KB\n",
      "Pruned + quantized size    : 281.05 KB\n",
      "PQ vs baseline reduction   : 14.1%\n",
      "PQ vs quantized reduction  : 0.0%\n",
      "PQ vs pruned reduction     : 14.1%\n"
     ]
    }
   ],
   "source": [
    "# Compare model sizes: baseline, quantized, pruned, pruned+quantized\n",
    "\n",
    "import os\n",
    "\n",
    "BASELINE_MODEL_PATH      = os.path.join(\"..\", \"models\", \"har_baseline.onnx\")\n",
    "QUANT_MODEL_PATH         = os.path.join(\"..\", \"models\", \"har_quantized.onnx\")\n",
    "PRUNED_MODEL_PATH        = os.path.join(\"..\", \"models\", \"har_pruned.onnx\")\n",
    "PRUNED_QUANT_MODEL_PATH  = os.path.join(\"..\", \"models\", \"har_pruned_quantized.onnx\")\n",
    "\n",
    "size_base_kb  = os.path.getsize(BASELINE_MODEL_PATH)     / 1024\n",
    "size_quant_kb = os.path.getsize(QUANT_MODEL_PATH)        / 1024\n",
    "size_prun_kb  = os.path.getsize(PRUNED_MODEL_PATH)       / 1024\n",
    "size_pq_kb    = os.path.getsize(PRUNED_QUANT_MODEL_PATH) / 1024\n",
    "\n",
    "print(f\"Baseline model size        : {size_base_kb:.2f} KB\")\n",
    "print(f\"Quantized model size       : {size_quant_kb:.2f} KB\")\n",
    "print(f\"Pruned model size          : {size_prun_kb:.2f} KB\")\n",
    "print(f\"Pruned + quantized size    : {size_pq_kb:.2f} KB\")\n",
    "\n",
    "print(f\"PQ vs baseline reduction   : {100 * (1 - size_pq_kb / size_base_kb):.1f}%\")\n",
    "print(f\"PQ vs quantized reduction  : {100 * (1 - size_pq_kb / size_quant_kb):.1f}%\")\n",
    "print(f\"PQ vs pruned reduction     : {100 * (1 - size_pq_kb / size_prun_kb):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c153e-4d91-4f43-9d7f-4ac6fd57746d",
   "metadata": {},
   "source": [
    "### 5.4 - Accuracy of pruned + quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04263a6b-f382-4d1b-a1c7-12cf60b8ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy        : 92.98%\n",
      "Quantized accuracy       : 93.04%\n",
      "Pruned accuracy          : 89.76%\n",
      "Pruned + quantized acc.  : 89.81%\n",
      "PQ vs baseline change    : -3.17 percentage points\n"
     ]
    }
   ],
   "source": [
    "# 5.4 – Accuracy comparison including pruned + quantized model\n",
    "\n",
    "def predict_with_session(sess, in_name, out_name, X: np.ndarray) -> np.ndarray:\n",
    "    preds = []\n",
    "    for window in X:\n",
    "        out = sess.run([out_name], {in_name: window[None, ...]})[0]\n",
    "        preds.append(out.argmax())\n",
    "    return np.array(preds)\n",
    "\n",
    "# Baseline\n",
    "y_pred_base = predict_with_session(session,  input_name,  output_name,  X_bench)\n",
    "acc_base    = (y_pred_base == y_bench).mean() * 100.0\n",
    "\n",
    "# Quantized\n",
    "y_pred_quant = predict_with_session(session_q, q_input_name, q_output_name, X_bench)\n",
    "acc_quant    = (y_pred_quant == y_bench).mean() * 100.0\n",
    "\n",
    "# Pruned\n",
    "y_pred_prun = predict_with_session(session_p, p_input_name, p_output_name, X_bench)\n",
    "acc_prun    = (y_pred_prun == y_bench).mean() * 100.0\n",
    "\n",
    "# Pruned + Quantized\n",
    "y_pred_pq = predict_with_session(session_pq, pq_input_name, pq_output_name, X_bench)\n",
    "acc_pq    = (y_pred_pq == y_bench).mean() * 100.0\n",
    "\n",
    "print(f\"Baseline accuracy        : {acc_base:.2f}%\")\n",
    "print(f\"Quantized accuracy       : {acc_quant:.2f}%\")\n",
    "print(f\"Pruned accuracy          : {acc_prun:.2f}%\")\n",
    "print(f\"Pruned + quantized acc.  : {acc_pq:.2f}%\")\n",
    "\n",
    "print(f\"PQ vs baseline change    : {acc_pq - acc_base:.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80af8b-51e3-440b-859f-0dc959efa38f",
   "metadata": {},
   "source": [
    "### 5.5 - Latency of pruned + quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a3f7a6b-c3b5-4e8e-9707-3f1f40377eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline latency        : 0.158 ms\n",
      "Quantized latency       : 0.138 ms\n",
      "Pruned latency          : 0.136 ms\n",
      "Pruned + quantized lat. : 0.140 ms\n",
      "PQ vs baseline speed-up : 11.3%\n",
      "PQ vs quantized speed-up: -1.1%\n",
      "PQ vs pruned speed-up   : -2.4%\n"
     ]
    }
   ],
   "source": [
    "# 5.5 – Latency comparison including pruned + quantized model\n",
    "\n",
    "import time\n",
    "\n",
    "def measure_latency_session(sess, in_name, out_name, samples: np.ndarray, runs: int = 20) -> float:\n",
    "    # Warm-up\n",
    "    for _ in range(10):\n",
    "        _ = sess.run([out_name], {in_name: samples[0:1]})\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        for window in samples:\n",
    "            _ = sess.run([out_name], {in_name: window[None, ...]})\n",
    "    end = time.time()\n",
    "\n",
    "    num_calls = runs * len(samples)\n",
    "    return (end - start) * 1000.0 / num_calls\n",
    "\n",
    "# Use first 100 benchmark windows\n",
    "samples = X_bench[:100]\n",
    "\n",
    "lat_base = measure_latency_session(session,  input_name,  output_name,  samples, runs=20)\n",
    "lat_quant = measure_latency_session(session_q, q_input_name, q_output_name, samples, runs=20)\n",
    "lat_prun  = measure_latency_session(session_p, p_input_name, p_output_name, samples, runs=20)\n",
    "lat_pq    = measure_latency_session(session_pq, pq_input_name, pq_output_name, samples, runs=20)\n",
    "\n",
    "print(f\"Baseline latency        : {lat_base:.3f} ms\")\n",
    "print(f\"Quantized latency       : {lat_quant:.3f} ms\")\n",
    "print(f\"Pruned latency          : {lat_prun:.3f} ms\")\n",
    "print(f\"Pruned + quantized lat. : {lat_pq:.3f} ms\")\n",
    "\n",
    "print(f\"PQ vs baseline speed-up : {100 * (1 - lat_pq / lat_base):.1f}%\")\n",
    "print(f\"PQ vs quantized speed-up: {100 * (1 - lat_pq / lat_quant):.1f}%\")\n",
    "print(f\"PQ vs pruned speed-up   : {100 * (1 - lat_pq / lat_prun):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf786e7-d985-4a5a-99de-eb07696ae7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:unit21_5G]",
   "language": "python",
   "name": "conda-env-unit21_5G-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
